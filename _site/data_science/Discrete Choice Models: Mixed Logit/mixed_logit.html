<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Mixed Logit | saubhik</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Mixed Logit" />
<meta name="author" content="Saubhik Mukherjee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal cyber-space dump." />
<meta property="og:description" content="Personal cyber-space dump." />
<link rel="canonical" href="http://localhost:4000/data_science/Discrete%20Choice%20Models:%20Mixed%20Logit/mixed_logit.html" />
<meta property="og:url" content="http://localhost:4000/data_science/Discrete%20Choice%20Models:%20Mixed%20Logit/mixed_logit.html" />
<meta property="og:site_name" content="saubhik" />
<script type="application/ld+json">
{"description":"Personal cyber-space dump.","author":{"@type":"Person","name":"Saubhik Mukherjee"},"@type":"WebPage","url":"http://localhost:4000/data_science/Discrete%20Choice%20Models:%20Mixed%20Logit/mixed_logit.html","headline":"Mixed Logit","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=7f90a6ed2a8ee84cfa7f3ceb5a300c7e86b460f3">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">saubhik</a></h1>

        

        <p>Personal cyber-space dump.</p>

        <p class="view"><a href="https://github.com/saubhik/">View my GitHub profile<small>github.com/saubhik</small></a></p>
        <p class="view"><a href="/posts.html">Blog<small>/posts</small></a></p>

        

        <a href="/index.html">Home</a>
</br><a href="/papers/">Comments on deep learning papers</a>
</br><a href="/epi/">Elements Of Programming Interviews</a>
</br><a href="/software_development/">Software Development</a>
</br><a href="/utilities/">Utilities</a>
</br><a href="/data_science/">Data Science</a>
</br><a href="/notebooks/">Jupyter Notebooks</a>
</br><a href="/deep_learning/">Deep Learning</a>
</br><a href="/coursework/index.html">CS courses notes</a>
</br><a href="/this_site/">About this website</a>
</br><a href="/practice-javascript/">Practice Javascript</a>


      </header>
      <section>

      <h1 id="mixed-logit">Mixed Logit</h1>

<p>Some notes for people already familiar with multinomial logit model
also known as softmax regression or multinomial logistic models.</p>

<p>These models are known as Discrete Choice Models, mostly studied
in Econometrics.</p>

<p>These notes are from Kenneth Train’s book on <a href="https://eml.berkeley.edu/books/choice2.html">Discrete Choice Methods
with Simulation</a>.</p>

<hr />

<p>Three limitations of standard logit:</p>

<ol>
  <li>
    <p>Random taste variation</p>
  </li>
  <li>
    <p>Unrestricted substitution patterns</p>
  </li>
  <li>
    <p>Correlation in unobserved factors over time</p>
  </li>
</ol>

<p>Choice probabilities are given by:
<script type="math/tex">P_{ni} = \int L_{ni}(\beta)f(\beta)d\beta</script>
is the probability of respondent $n$ choosing product $i$.</p>

<p>$L_{ni}(\beta) = \frac{\exp(V_{ni}(\beta))}{\sum_{j}\exp(V_{nj}(\beta))}$ is called the logit probability at $\beta$.</p>

<p>We could have $V_{ni}(\beta) = {\beta}’{x_{ni}}$.</p>

<p>This form of $P_{ni}$ is called a <strong>mixed</strong> form, with $f(.)$ as the <strong>mixing distribution</strong>.
It is like taking the weighted average of the logit probabilities, $L_{ni}$ at different
$\beta$s, with $f(\beta)$ as the weights. Hence, the name <strong>mixed logit</strong>.</p>

<p>Note that we get standard logit when $f(\beta) = I(\beta = b)$. So this is more general.</p>

<p>We could take $f(\beta) = \phi(\beta | b, W)$. This is normal density with mean vector $b$
and covariance matrix $W$.</p>

<p>In Hierarchical Bayesian estimation techniques, we assume prior distributions for $b$ and 
$W$. There is a hierarchy of priors. And we use Bayes’ Theorem. Hence the name <strong>Hierarchical Bayes</strong>.</p>

<h3 id="motivations">Motivations</h3>
<h4 id="random-coefficients">Random Coefficients</h4>
<p>Decision maker $n$ faces a choice task among $J$ alternatives. We model utility that $n$ gets from alternative $j$ as <script type="math/tex">U_{nj} = \beta_{n}'x_{nj} + \epsilon_{nj}</script> where 
$\epsilon_{nj}$ are i.i.d extreme value.</p>

<p>Decision maker knows $\beta_{n}$ and $\epsilon_{nj}$ for all $j$, and chooses alternative
$i$ that maximises his utility from $i$, $U_{ni} &gt; U_{nj}$ for all $j \neq i$.</p>

<p>Researcher doesn’t observe $\beta_{n}$ and $\epsilon_{nj}$, but observes $x_{nj}$. So he 
does the math and finds out:
<script type="math/tex">P_{ni} = P(U_{ni} > U_{nj} \forall j \neq i) = \int L_{ni}(\beta)f(\beta)d\beta.</script></p>

<h4 id="error-components">Error Components</h4>
<p>Utility is modeled as <script type="math/tex">U_{ni} = \alpha'x_{nj} + \eta_{nj}</script> where $\eta_{nj}$ is unobserved
part of the utility.
It is natural that errors $\eta_{nj}$ won’t be independent over respondents, $n$ and also
over the alternatives $j$, because obviously we cannot observe <em>everything</em>.
So, we introduce correlation, <script type="math/tex">\eta_{nj} = \mu_{n}'z_{nj} + \epsilon_{nj}</script> where $\mu_{n}$ are random terms with mean $0$ and $\epsilon_{nj}$ are i.i.d. extreme value.</p>

<p>$\alpha$ is a vector of fixed coefficients. $z_{nj}$ could be $x_{nj}$.</p>

<p>Some math:
<script type="math/tex">Cov(\eta_{ni}, \eta_{nj}) = \mathbb{E}[(\mu_{n}'z_{ni} + \epsilon_{ni})(\mu_{n}'z_{nj} + \epsilon_{nj})] = z_{ni}'Wz_{nj}</script> where W is the covariance matrix of $\mu_{n}$. This shows that the unobserved utility components are correlated over alternatives.</p>

<p>See that even when $W$ is diagonal, i.e. the $\mu_{nj}$ are independent over $j$, the covariance above is not $0$.</p>

<p>You get the standard logit when you take $z_{nj} = 0$. There is no correlation in utility among alternatives. This gives rise to IIA property and its restrictive substitution patterns.</p>

<p>To get nested logit with $K$ nests, take $\mu_{n}’z_{nj} = \sum_{k}\mu_{nk}I(j \in k)$. That is $z_{nj}$ will be a $K$ dimensional binary vector, with only one $1$ at the position corresponding to the nest it belongs to. We take $\mu_{nk}$ to be $N(0, \sigma_k)$, all independent over $n, k$. This $\mu_{nk}$ enters into utility of each alternative in nest $k$ inducing correlation among alternatives in $k$.</p>

<p>But $\mu_{nk}$ does not enter into utility of any alternative in other nests. So we have
IIN and its restrictive substitution patterns.</p>

<p>So mixed logit is more general.</p>

<h3 id="substitution-patterns">Substitution Patterns</h3>
<p>There is no IIA or IIN.</p>

<p>Percentage change in the probability for one alternative $i$ given a percentage change in the $m$th attribute of another alternative $j$ is
<script type="math/tex">E_{nix_{nj}^{m}} = -x_{nj}^{m}\int{\beta^{m}L_{nj}(\beta)\frac{L_{ni}(\beta)}{P_{ni}}f(\beta)d\beta}.</script></p>

<p>This elasticity is different for each $i$. A $10\%$ reduction in probability for one alternative need not imply a $10\%$ reduction in probability for each other alternative, unlike logit.</p>

<h3 id="simulation">Simulation</h3>
<p>The choice probabilities are:
<script type="math/tex">P_{ni} = \int L_{ni}(\beta)f(\beta | \theta)d\beta</script>
and <script type="math/tex">L_{ni}(\beta) = \frac{\exp(V_{ni}(\beta))}{\sum_{j}\exp(V_{nj}(\beta))}.</script></p>

<p>For any given value of $\theta$.</p>

<ol>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Draw a value of $\beta$ from $f(\beta</td>
          <td>\theta)$. Call it $\beta^{r}$ for $r$th draw.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Calculate $L_{ni}(\beta^{r})$.</p>
  </li>
  <li>Repeat steps 1 and 2 many times, say $R$ times, and average the results.</li>
</ol>

<p>So we are using Law of Large Numbers.</p>

<p>The average is the simulated probability: <script type="math/tex">\hat{P}_{ni} = \frac{1}{R}\sum_{r}L_{ni}(\beta^{r}).</script></p>

<p>We now maximise the SLL, the simulated log-likelihood: <script type="math/tex">SLL = \sum_{n}\sum_{j}d_{nj}\log{\hat{P}_{nj}}</script> where $d_{nj}$ is $1$ if $n$ chose alternative $j$, otherwise $0$.</p>

<p>The $\theta$ which maximises the SLL, is the MSLE, Maximum Simulated Likelihood Estimator.</p>

<p>But we cannot use this estimation for our projects. Think about what would happen when we consider new price scenarios.</p>

<p>Read the case study (page 147, page 14 of the PDF) <a href="https://eml.berkeley.edu/books/choice2nd/Ch06_p134-150.pdf">here</a>, for a practical example.</p>

<hr />

<h2 id="hierarchical-bayesian-estimation">Hierarchical Bayesian estimation</h2>

<p>Let the utility that person $n$ obtains from alternative $j$ in time period $t$ be
<script type="math/tex">U_{njt} = \beta_{n}'x_{njt} + \epsilon_{njt}</script> where $\epsilon_{njt}$ is i.i.d. extreme value and $\beta_{n}$ follows $N(b, W)$.</p>

<p>There is a normal prior on $b$ with large variance.
There is an inverted wishart prior on $W$.
$b$ and $W$ are called hyper-parameters.</p>

<p>We observe a sample of $N$ people. The chosen alternatives for person $n$ is $y_n’ = (y_{n1}, …, y_{nT})$. The choices of the entire sample is $Y = (y_1, …, y_N)$.</p>

<p>The probability of person $n$’s observed choices, conditional on $\beta$ is
<script type="math/tex">L(y_n | \beta) = \prod_{t}{\frac{exp(\beta'x_{ny_{nt}t})}{\sum_{j}exp(\beta'x_{njt})}}.</script></p>

<p>The unconditional probability of person $n$’s observed choices is
<script type="math/tex">L(y_n | b, W) = \int L(y_n | \beta) \phi(\beta | b, W) d\beta.</script></p>

<p>This is the mixed logit probability, with $f(.)$ being the normal density.</p>

<p>Now, use Bayes’ Theorem,
<script type="math/tex">K(b, W | Y) \propto \prod_{n} L(y_n | b, W) k(b, W)</script>
where $k(b, W)$ is the product of normal density for $b$ and the inverted wishart for $W$. That is, the product of priors.</p>

<table>
  <tbody>
    <tr>
      <td>We want to draw from the <strong>posterior</strong> distribution $K(b, W</td>
      <td>Y)$. So that we can average the draws to get the estimates for $b$ and $W$, using Law of Large Numbers.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>But $L(y_n</td>
      <td>b, W)$ is an expression involving an integral and it does not have a closed form and therfore, must be approximated through simulation. This makes estimation using MCMC algorithms like Metropolis-Hastings very time consuming.</td>
    </tr>
  </tbody>
</table>

<p>That is why we use this set up:
<script type="math/tex">K(b, W, \beta_{n} \forall n | Y) \propto \prod_{n} L(y_n | \beta_n) \phi(\beta_n | b, W) k(b, W).</script></p>

<table>
  <tbody>
    <tr>
      <td>Here, $L(y_n</td>
      <td>\beta_n)$ is the logit formula.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Then we use Gibbs sampling. We keep drawing $b, W, \beta_n \forall n$, one at a time, conditioning the others. The resulting draws converge to draws from the joint posterior $K(b, W, \beta_{n} \forall n</td>
      <td>Y)$.</td>
    </tr>
  </tbody>
</table>

<p>Once the converged draws are obtained, the mean and standard deviation of the draws can be calculated. This gives the estimates and standard errors of the parameters.</p>

<p>This procedure also gives $\beta_n \forall n$. So this estimation technique can be used for our projects where we keep changing pricing scenarios.</p>

<p>Thumb rule: A total of $20000$ iterations can be used for $1000$ draws: $10000$ for burn-in and $10000$ after convergence, of which every 10th draw is retained to calculate the estimates and standard errors to remove correlations over iterations.</p>

<p>Read <a href="https://cran.r-project.org/web/packages/bayesm/vignettes/Constrained_MNL_Vignette.html">this</a> for the prior used in <code class="highlighter-rouge">bayesm</code> package.</p>

<hr />

<h3 id="remaining-stuff">Remaining stuff:</h3>
<p>Implementation of the following have not yet been done:</p>

<ul>
  <li>
    <p>Determining whether convergence has achieved</p>
  </li>
  <li>
    <p>MCMC diagnostics like Gelman-Rubin test statistic</p>
  </li>
  <li>
    <p>More performance diagnostics (as mentioned <a href="http://rsginc.com/files/news/Understanding%20How%20Covariates%20Perform%20Across%20Different%20HB%20Packages%20-%20Poster.pdf">here</a>)</p>
  </li>
</ul>

<hr />


      </section>
      <footer>
        <p><small>Love &mdash; Peace &mdash; Code</small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>